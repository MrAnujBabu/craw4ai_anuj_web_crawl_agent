import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai import JsonCssExtractionStrategy
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter


async def example_1_basic_crawl():
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 1: åŸºç¡€çˆ¬å–")
    print("="*60)
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com")
        print(f"æˆåŠŸ: {result.success}")
        print(f"æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
        print(f"Markdown é•¿åº¦: {len(result.markdown)} å­—ç¬¦")
        print(f"Markdown å†…å®¹é¢„è§ˆ:\n{result.markdown[:200]}...")


async def example_2_clean_content():
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 2: æ¸…ç†å†…å®¹ï¼ˆç§»é™¤å¯¼èˆªã€é¡µè„šç­‰ï¼‰")
    print("="*60)
    
    config = CrawlerRunConfig(
        excluded_tags=["nav", "footer", "aside"],
        remove_overlay_elements=True,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.48)
        ),
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://en.wikipedia.org/wiki/Python", config=config)
        print(f"æ¸…ç†å Markdown é•¿åº¦: {len(result.markdown)} å­—ç¬¦")
        print(f"å†…å®¹é¢„è§ˆ:\n{result.markdown[:300]}...")


async def example_3_css_extraction():
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 3: CSS é€‰æ‹©å™¨æå–ç»“æ„åŒ–æ•°æ®")
    print("="*60)
    
    schema = {
        "name": "News Articles",
        "baseSelector": "article",
        "fields": [
            {"name": "title", "selector": "h2, h3", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
        ]
    }
    
    config = CrawlerRunConfig(
        extraction_strategy=JsonCssExtractionStrategy(schema),
        css_selector="article",
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=config)
        print(f"æå–æˆåŠŸ: {result.success}")
        if result.extracted_content:
            print(f"æå–çš„å†…å®¹:\n{result.extracted_content[:300]}...")


async def example_4_javascript_execution():
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 4: æ‰§è¡Œ JavaScript")
    print("="*60)
    
    config = CrawlerRunConfig(
        js_code="document.body.style.backgroundColor = '#f0f0f0';",
        delay_before_return_html=0.5,
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=config)
        print(f"JavaScript æ‰§è¡ŒæˆåŠŸ: {result.success}")
        print(f"é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")


async def example_5_screenshot():
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 5: æˆªå›¾")
    print("="*60)
    
    config = CrawlerRunConfig(screenshot=True)
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=config)
        print(f"æˆªå›¾æˆåŠŸ: {result.success}")
        if result.screenshot:
            print(f"æˆªå›¾æ•°æ®é•¿åº¦: {len(result.screenshot)} å­—èŠ‚")
            import base64
            with open("example_screenshot.png", "wb") as f:
                f.write(base64.b64decode(result.screenshot))
            print("æˆªå›¾å·²ä¿å­˜åˆ°: example_screenshot.png")


async def example_6_links_analysis():
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 6: é“¾æ¥åˆ†æ")
    print("="*60)
    
    config = CrawlerRunConfig(
        exclude_external_links=False,
        exclude_social_media_links=False,
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=config)
        internal_links = result.links.get("internal", [])
        external_links = result.links.get("external", [])
        
        print(f"å†…éƒ¨é“¾æ¥æ•°é‡: {len(internal_links)}")
        print(f"å¤–éƒ¨é“¾æ¥æ•°é‡: {len(external_links)}")
        
        if internal_links:
            print("\nå†…éƒ¨é“¾æ¥ç¤ºä¾‹:")
            for link in internal_links[:3]:
                print(f"  - {link.get('href', 'N/A')}")


async def main():
    print("\n" + "ğŸš€ Crawl4AI åŸºç¡€ç¤ºä¾‹æ¼”ç¤º ğŸš€")
    print("="*60)
    
    await example_1_basic_crawl()
    await example_2_clean_content()
    await example_3_css_extraction()
    await example_4_javascript_execution()
    await example_5_screenshot()
    await example_6_links_analysis()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    asyncio.run(main())
